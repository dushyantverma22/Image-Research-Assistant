Base64 in Simple Words üç™üì¶
Think of it like this: You're sending chocolate chip cookies through a mail tube that only accepts marbles.

The Problem:
Cookies don't fit through the tube

The tube only accepts small, round marbles

But you really need to send your cookie recipe!

The Solution (Base64):
Break the cookie into crumbs (your original data)

Put 3 crumbs into each of 4 small boxes (convert binary to groups of 6 bits)

Label each box with a marble color (each 6-bit group = 1 printable character)

Send the marbles through the tube (safe transmission)

On the other side, reverse the process (decode back to original)

Key Points for Interview:
What it is:
A way to represent binary data (images, files) using only 64 safe characters: A-Z, a-z, 0-9, +, /

Makes "unsafe" data safe for emails, URLs, and text-only systems

Why we need it:
Some systems only understand text, not raw binary

Prevents special characters from breaking things

Like putting a fragile gift in a protective box before shipping

Simple Example:
text
Original: "Hi"
‚Üí Binary: 01001000 01101001
‚Üí Base64: "SGk="
The "=" at the end:
Just padding (like adding empty boxes to fill the shipment)

Makes sure we always have complete groups

Perfect Interview Answer:
"Base64 is like a universal packaging system. When data needs to travel through systems that only understand text, we convert it into a safe text format using 64 common characters. It's not encryption (no secret key), just repackaging for safe delivery."

Real-life uses:
Email attachments

Embedding images in HTML/CSS

API tokens

Storing binary data in JSON/XML

Remember: It makes data 33% larger (like adding bubble wrap), but ensures it arrives intact! üì¶‚ú®


========================

Q1. Why converted to Base64?
Ans: then transforms that data into a standardized format suitable for API transmission,

import asyncio
import os
from typing import Annotated

from dotenv import load_dotenv
from typing_extensions import TypedDict

from langchain_core.messages import AnyMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition

from langchain_mcp_adapters.client import MultiServerMCPClient

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

server_configs = {
    "vision": {
        "command": "python",
        "args": ["visual_analysis_server.py"],
        "transport": "stdio",
    }
}

class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

def create_graph(tools: list):
    if not OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY not found in env")

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        api_key=OPENAI_API_KEY,
    )

    # get_tools() already returns LangChain BaseTool. [web:44]
    llm_with_tools = llm.bind_tools(tools, tool_choice="any")

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are an image analysis assistant.\n"
                "When the user gives an image path, you MUST:\n"
                "1) Call load_image_from_path(file_path=...) to get base64_image and mime_type.\n"
                "2) Call get_image_description(base64_image_string=..., mime_type=...).\n"
                "3) Return the description text to the user.\n"
                "If the file cannot be read, mention the error to the user.\n"
            ),
            MessagesPlaceholder("messages"),
        ]
    )


    chat_llm = prompt | llm_with_tools

    def chat_node(state: State):
        response = chat_llm.invoke({"messages": state["messages"]})
        # Return ONLY the delta; add_messages will append.
        return {"messages": [response]}

    builder = StateGraph(State)
    builder.add_node("chat", chat_node)

    # ToolNode must be named "tools" for tools_condition conventions. [web:6]
    builder.add_node("tools", ToolNode(tools))

    builder.add_edge(START, "chat")

    # Explicit mapping avoids odd routing behavior. [web:59]
    builder.add_conditional_edges(
        "chat",
        tools_condition,
        {
            "tools": "tools",
            "__end__": END,
        },
    )

    builder.add_edge("tools", "chat")

    return builder.compile(checkpointer=MemorySaver())

async def main():
    client = MultiServerMCPClient(server_configs)
    tools = await client.get_tools()  # list[BaseTool] [web:44][web:3]
    print("Loaded tools:", [t.name for t in tools])
    agent = create_graph(tools)

    while True:
        user_input = input("\nYou: ").strip()
        if user_input.lower() in {"exit", "quit"}:
            break

        try:
            out = await agent.ainvoke(
                {"messages": [("user", user_input)]},
                config={"configurable": {"thread_id": "multi-server-session"}},
            )


            last = out["messages"][-1]
            print("AI content:", repr(last.content))
            print("AI tool_calls:", getattr(last, "tool_calls", None))

            # Optional: show tool calls if present
            if hasattr(last, "tool_calls") and last.tool_calls:
                print("Tool calls:", last.tool_calls)

            print("AI:", out["messages"][-1].content)
        except Exception as e:
            print("Error:", e)

if __name__ == "__main__":
    asyncio.run(main())



========================================

# mcp_client.py
import asyncio
import os
from typing import Annotated

import gradio as gr
from dotenv import load_dotenv
from typing_extensions import TypedDict

from langchain_core.messages import AnyMessage, ToolMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import tools_condition, ToolNode

from langchain_mcp_adapters.client import MultiServerMCPClient


# ------------------------------------------------------------------
# ENV SETUP
# ------------------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("‚ùå OPENAI_API_KEY not found in environment variables")


# ------------------------------------------------------------------
# MCP SERVER CONFIG
# ------------------------------------------------------------------
server_configs = {
    "vision": {
        "command": "python",
        "args": ["visual_analysis_server.py"],
        "transport": "stdio",
    },
    "wikipedia": {
        "command": "python",
        "args": ["research_server.py"],
        "transport": "stdio",
    }
}


# ------------------------------------------------------------------
# STATE DEFINITION
# ------------------------------------------------------------------
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]


# ------------------------------------------------------------------
# MCP RESPONSE PARSER (IMPORTANT)
# ------------------------------------------------------------------
def extract_text_from_mcp_result(result):
    """
    Handles all known MCP response formats and safely extracts text.
    """
    if isinstance(result, list):
        for item in result:
            if isinstance(item, dict):
                if item.get("type") == "text":
                    return item.get("text", "")
                if "text" in item:
                    return item["text"]
    if isinstance(result, dict):
        if "text" in result:
            return result["text"]
        if "content" in result and isinstance(result["content"], str):
            return result["content"]
    return str(result)


# ------------------------------------------------------------------
# CUSTOM TOOL NODE (CRITICAL FIX)
# ------------------------------------------------------------------
class FixedToolNode(ToolNode):
    async def _arun_tool(self, tool_call, state):
        try:
            result = await super()._arun_tool(tool_call, state)
            extracted = extract_text_from_mcp_result(result)

            return ToolMessage(
                content=extracted,
                tool_call_id=tool_call["id"],
                name=tool_call["name"]
            )

        except Exception as e:
            return ToolMessage(
                content=f"Tool execution error: {str(e)}",
                tool_call_id=tool_call["id"],
                name=tool_call["name"]
            )


# ------------------------------------------------------------------
# GRAPH CREATION
# ------------------------------------------------------------------
def create_graph(tools: list):

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
        api_key=OPENAI_API_KEY,
    )

    llm_with_tools = llm.bind_tools(tools)

    # üî• STRICT AGENTIC PROMPT (UNCHANGED)
    prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            "You are an agentic AI system.\n"
            "You MUST follow this workflow strictly:\n"
            "1. If user mentions an image path, call the vision tool to extract the main topic.\n"
            "2. Then call the Wikipedia tool using that topic.\n"
            "3. Return a fact-based explanation using Wikipedia data only.\n"
            "Do NOT skip steps. Do NOT hallucinate. Always use tools."
        ),
        MessagesPlaceholder("messages"),
    ])

    chat_llm = prompt | llm_with_tools

    def chat_node(state: State):
        response = chat_llm.invoke({"messages": state["messages"]})
        return {"messages": [response]}

    tool_node = FixedToolNode(tools)

    builder = StateGraph(State)

    builder.add_node("chat", chat_node)
    builder.add_node("tools", tool_node)

    builder.add_edge(START, "chat")

    builder.add_conditional_edges(
        "chat",
        tools_condition,
        {
            "tools": "tools",
            "__end__": END,
        },
    )

    builder.add_edge("tools", "chat")

    return builder.compile(checkpointer=MemorySaver())


# ------------------------------------------------------------------
# AGENT SETUP (RUNS ONCE)
# ------------------------------------------------------------------
async def setup_agent():
    print("üöÄ Initializing MCP Client & Tools...")
    client = MultiServerMCPClient(server_configs)
    tools = await client.get_tools()

    print(f"‚úÖ Loaded {len(tools)} tools:")
    for tool in tools:
        print(f"  - {tool.name}")

    agent = create_graph(tools)
    print("ü§ñ Agent is READY")
    return agent


agent = asyncio.run(setup_agent())


# ------------------------------------------------------------------
# GRADIO UI
# ------------------------------------------------------------------
with gr.Blocks(theme=gr.themes.Default(primary_hue="blue")) as demo:
    gr.Markdown("# üß† Image Research Assistant (MCP + LangGraph)")

    chatbot = gr.Chatbot(height=500)



    with gr.Row():
        image_box = gr.Image(type="filepath", label="Upload Image")
        text_box = gr.Textbox(
            label="Ask a question about the image or research topic",
            placeholder="e.g. Describe this image and explain its historical importance",
            scale=2
        )

    submit_btn = gr.Button("Submit", variant="primary")


    async def get_agent_response(user_text, image_path, chat_history):
        if chat_history is None:
            chat_history = []

        # Build user message
        if image_path:
            full_message = f"{user_text}\n\nImage path: {image_path}"
            chat_history.append({
                "role": "user",
                "content": f"üì∑ {image_path}\n{user_text}"
            })
        else:
            full_message = user_text
            chat_history.append({
                "role": "user",
                "content": user_text
            })

        try:
            result = await agent.ainvoke(
                {
                    "messages": [
                        HumanMessage(content=full_message)
                    ]
                },
                config={"configurable": {"thread_id": "gradio-session"}}
            )

            last_message = result["messages"][-1]
            bot_reply = last_message.content if last_message.content else "‚ö†Ô∏è No response generated."

        except Exception as e:
            bot_reply = f"‚ùå Error: {str(e)}"

        chat_history.append({
            "role": "assistant",
            "content": bot_reply
        })

        return "", chat_history, None



    submit_btn.click(
        get_agent_response,
        inputs=[text_box, image_box, chatbot],
        outputs=[text_box, chatbot, image_box]
    )


demo.launch(server_name="Localhost", server_port=7860)
